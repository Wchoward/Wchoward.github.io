---
title: 2021-01-14-Word2Vec详解
date: 2021-01-14 16:45:22
tags:
- Word2vector
- 分词
- Negative Sampling
categories:
- Deep Learning
- NLP
---

# Word2Vector 词向量

word2vec工具主要包含两个模型：

- 跳字模型（skip-gram）
- 连续词袋模型（continuous bag of words，简称CBOW），

以及两种高效训练的方法：

- 负采样（negative sampling）
- 层序softmax（hierarchical softmax）。

## CBOW & Skip-Gram

### CBOW

输入：n个词向量 context词

输出：所有词的Softmax概率 （训练的目标是期望训练样本特定词对应的softmax概率最大） focus word



### Skip-Gram

输入：特定词 fucus word

输出：特定词对应的上下文词向量



## Huffman Tree

高频词靠近树根，需要更少的时间即可被找到，符合贪心优化思想。

采用二元逻辑回归方法，沿左子树走->负类（哈夫曼编码1），沿右子树走->正类（哈夫曼编码0）。判别方法：sigmoid:   $P(+) = \sigma(x^T_w\theta) = \frac{1}{1 + e ^ {-x^T_w \theta}}$

目标：找到合适的所有节点的词向量和所有内部节点$\theta$, 使训练样本达到最大似然。

## Negative Sampling

我们有一个训练样本，中心词是$w$,它周围上下文共有$2c$个词，记为$context(w)$。由于这个中心词$w$,的确和$context(w)$相关存在，因此它是一个真实的正例。通过Negative Sampling采样，我们得到neg个和$w$不同的中心词$w_i, i=1,2,..neg$，这样$context(w)$和$w_i$就组成了neg个并不真实存在的负例。利用这一个正例和neg个负例，我们进行二元逻辑回归，得到负采样对应每个词对应的模型参数$\theta_i$，和每个词的词向量。

